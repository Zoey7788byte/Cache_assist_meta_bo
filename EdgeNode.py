import torch.nn as nn
from typing import List, Dict, Tuple, Any
from collections import OrderedDict, defaultdict
from cloud_manager import CloudModelManager
import os
import torch
import heapq
from PIL import Image
import torchvision.transforms as transforms
import traceback
import shutil
import hashlib
from generate_pt.ModelArchitecture import MODEL_CONFIGS, create_model

class EdgeNode:
    def __init__(self, node_id: str, cloud_manager: CloudModelManager, 
                 max_storage: float = 1024.0, cache_dir: str = "./model_data/edge_cache_model"):
        self.node_id = node_id
        self.cloud_manager = cloud_manager
        self.max_storage = max_storage  # å•ä½ï¼šMB
        self.cache_dir = cache_dir
        self.used_storage = 0.0
        self.model_cache = {} #åˆå§‹åŒ–æ¨¡å‹å­—å…¸
        os.makedirs(self.cache_dir, exist_ok=True)

    def Initialize_cache(self): 
        # æ¸…ç©ºç¼“å­˜ç›®å½•å¹¶é‡ç½®å­˜å‚¨ä½¿ç”¨
        if os.path.exists(self.cache_dir):
            shutil.rmtree(self.cache_dir)
        os.makedirs(self.cache_dir, exist_ok=True)
        self.used_storage = 0
        
        # æ”¶é›†æ‰€æœ‰å±‚ï¼ˆå…±äº«å±‚å’Œç‰¹å®šå±‚ï¼‰çš„ä¿¡æ¯
        all_layers = defaultdict(lambda: {'count': 0, 'keys': [], 'size': 0, 'type': None})
        
        # æ„å»ºå±‚æ˜ å°„å¹¶ç»Ÿè®¡ä¿¡æ¯
        for registry_key, model_info in self.cloud_manager.model_db.items():
            # å¤„ç†å…±äº«å±‚
            if 'shared_path' in model_info:
                shared_path = model_info['shared_path']
                
                # åˆå§‹åŒ–æ–‡ä»¶å¤§å°ï¼ˆå¦‚æœå°šæœªè®°å½•ï¼‰
                if all_layers[shared_path]['size'] == 0:
                    if not os.path.exists(shared_path):
                        print(f"[Warn] Shared path missing: {shared_path}")
                        continue
                        
                    try:
                        file_size = os.path.getsize(shared_path) / (1024 * 1024)  # MB
                        if file_size <= 0:
                            file_size = 1e-9
                        all_layers[shared_path]['size'] = file_size
                        all_layers[shared_path]['type'] = 'shared'
                    except OSError:
                        print(f"[Error] Cannot get size for: {shared_path}")
                        continue
                
                # æ›´æ–°è®¡æ•°å’Œå…³è”é”®
                all_layers[shared_path]['count'] += 1
                all_layers[shared_path]['keys'].append(registry_key)
            
            # å¤„ç†ç‰¹å®šå±‚
            if 'specific_path' in model_info:
                specific_path = model_info['specific_path']
                
                # åˆå§‹åŒ–æ–‡ä»¶å¤§å°ï¼ˆå¦‚æœå°šæœªè®°å½•ï¼‰
                if all_layers[specific_path]['size'] == 0:
                    if not os.path.exists(specific_path):
                        print(f"[Warn] Specific path missing: {specific_path}")
                        continue
                        
                    try:
                        file_size = os.path.getsize(specific_path) / (1024 * 1024)  # MB
                        if file_size <= 0:
                            file_size = 1e-9
                        all_layers[specific_path]['size'] = file_size
                        all_layers[specific_path]['type'] = 'specific'
                    except OSError:
                        print(f"[Error] Cannot get size for: {specific_path}")
                        continue
                
                # ç‰¹å®šå±‚è®¡æ•°å§‹ç»ˆä¸º1ï¼ˆåªè¢«ä¸€ä¸ªæ¨¡å‹ä½¿ç”¨ï¼‰
                all_layers[specific_path]['count'] += 1
                all_layers[specific_path]['keys'].append(registry_key)

        # ç”Ÿæˆå€™é€‰åˆ—è¡¨ï¼ˆæŒ‰ä»·å€¼æ’åºï¼‰
        candidates = []
        for path, data in all_layers.items():
            # è·³è¿‡å¤§å°æœªåˆå§‹åŒ–æˆ–æ— æ•ˆçš„æ¡ç›®
            if data['size'] <= 0 or data['type'] is None:
                continue
                
            # ä»·å€¼å…¬å¼ï¼šä½¿ç”¨æ¬¡æ•° / å¤§å°
            value = data['count'] / data['size']
            candidates.append({
                'path': path,
                'value': value,
                'count': data['count'],
                'size': data['size'],
                'type': data['type'],
                'keys': data['keys']  # ä¿å­˜å…³è”çš„registry_keys
            })
        
        # æŒ‰ä»·å€¼é™åºæ’åº
        candidates.sort(key=lambda x: x['value'], reverse=True)
        
        # ç»Ÿä¸€ç¼“å­˜æ‰€æœ‰å±‚
        for candidate in candidates:
            # æ£€æŸ¥å­˜å‚¨ç©ºé—´
            if self.used_storage + candidate['size'] > self.max_storage:
                layer_type = candidate['type'].capitalize()
                print(f"[Skip] {layer_type} {os.path.basename(candidate['path'])} | "
                    f"Need: {candidate['size']:.1f}MB | "
                    f"Used: {self.used_storage:.1f}/{self.max_storage}MB")
                continue
            
            # åˆ›å»ºç¼“å­˜è·¯å¾„
            cache_filename = os.path.basename(candidate['path'])
            cache_path = os.path.join(self.cache_dir, cache_filename)
            
            # å®‰å…¨å¤åˆ¶æ–‡ä»¶
            storage_added = False
            try:
                if not os.path.exists(candidate['path']):
                    print(f"[Error] Source file disappeared: {candidate['path']}")
                    continue
                    
                if not os.path.exists(cache_path):
                    shutil.copy2(candidate['path'], cache_path)
                    
                    if not os.path.exists(cache_path) or os.path.getsize(cache_path) == 0:
                        raise RuntimeError("File copy failed")
                    
                    self.used_storage += candidate['size']
                    storage_added = True
                
                # æ›´æ–°æ‰€æœ‰å…³è”æ¨¡å‹çš„ç¼“å­˜ä¿¡æ¯
                for registry_key in candidate['keys']:
                    if registry_key not in self.model_cache:
                        model_info = self.cloud_manager.model_db[registry_key]
                        self.model_cache[registry_key] = {
                            'id': model_info['id'],
                            'filename': model_info['filename'],
                            'dataset': model_info['dataset'],
                            'model': model_info['model'],
                            'type': model_info['type'],
                            'accuracy': model_info.get('accuracy'),
                            'shared_cache_path': None,
                            'specific_cache_path': None
                        }
                    
                    # æ›´æ–°å¯¹åº”çš„ç¼“å­˜è·¯å¾„
                    if candidate['type'] == 'shared':
                        self.model_cache[registry_key]['shared_cache_path'] = cache_path
                    else:  # specific
                        self.model_cache[registry_key]['specific_cache_path'] = cache_path

            except Exception as e:
                layer_type = candidate['type'].capitalize()
                print(f"[Error] Cache {layer_type} failed: {str(e)}")
                if os.path.exists(cache_path):
                    try:
                        os.remove(cache_path)
                    except OSError:
                        pass
                if storage_added:
                    self.used_storage -= candidate['size']
        
        print(f"Cache completed | Used: {self.used_storage:.1f}MB/{self.max_storage}MB")

    def get_cache_status(self):
        """è·å–ç¼“å­˜ç©ºé—´ä½¿ç”¨æƒ…å†µ"""
        from collections import defaultdict
        stats = defaultdict(int)
        for layer_type in ['shared', 'specific']:
            dir_path = os.path.join(self.cache_dir, layer_type)
            if os.path.exists(dir_path):
                stats[layer_type] = sum(
                    os.path.getsize(os.path.join(dir_path, f)) 
                    for f in os.listdir(dir_path)
                ) / (1024*1024)  # MB
        return stats
         
    def prepare_inference(self, image_path: str) -> torch.Tensor:
        """å‡†å¤‡æ¨ç†è¾“å…¥æ•°æ®"""
        # å›¾åƒé¢„å¤„ç†
        transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        img = Image.open(image_path).convert('RGB')
        return transform(img).unsqueeze(0)  # æ·»åŠ batchç»´åº¦
  
    def inference(self, request_img: str, data: dict) -> dict:
        if request_img:
            input_tensor = self.prepare_inference(request_img)
            try:
                model_data = data['model_data']
                model_arch = data['model_arch']
                
                # è·å–æ¨¡å‹åœ¨æ•°æ®åº“ä¸­çš„å®Œæ•´ä¿¡æ¯
                registry_key = f"{model_data}|{model_arch}"
                model_info = self.cloud_manager.model_db.get(registry_key)
                if not model_info:
                    raise ValueError(f"Model {registry_key} not found in model database")

                # åŸºäºmodel_cacheè®¡ç®—ç¼“å­˜å®Œæ•´æ€§
                shared_cache_path = os.path.join(self.cache_dir, 'shared', os.path.basename(model_info['shared_path']))
                specific_cache_path = os.path.join(self.cache_dir, 'specific', os.path.basename(model_info['specific_path']))
                
                # ç›´æ¥ä»ç¼“å­˜å­—å…¸è·å–çŠ¶æ€
                shared_cached = shared_cache_path in self.model_cache
                specific_cached = specific_cache_path in self.model_cache
                completeness = (shared_cached + specific_cached) / 2  # å‡è®¾æ¨¡å‹ç”±ä¸¤éƒ¨åˆ†ç»„æˆ
                
                data['cache_status'] = "full" if completeness == 1 else "partial"
                
                # æ¨¡å‹ç»„è£…é€»è¾‘ï¼ˆæ ¹æ®ç¼“å­˜å…ƒæ•°æ®ä¼˜åŒ–ï¼‰
                if completeness == 1:
                    # ä»ç¼“å­˜è®°å½•ä¸­è·å–æ¨¡å‹ç‰¹å¾
                    shared_info = self.model_cache[shared_cache_path]
                    specific_info = self.model_cache[specific_cache_path]
                    
                    model = self._load_full_edge_model(
                        shared_path = shared_cache_path,
                        specific_path = specific_cache_path,
                        model_arch = model_arch,
                        specific_metadata = specific_info
                    )
                    data['model_assembly'] = "full_cache"
                else:
                    # æ™ºèƒ½ç»„è£…é€»è¾‘ï¼šä¼˜å…ˆä½¿ç”¨å·²ç¼“å­˜çš„ç»„ä»¶
                    cached_components = {
                        'shared': shared_cache_path if shared_cached else None,
                        'specific': specific_cache_path if specific_cached else None
                    }
                    registry_key = f"{model_data}|{model_arch}"
                    model = self._assemble_hybrid_model(cached_components, model_arch, registry_key)
                    data['model_assembly'] = f"hybrid_{completeness*100:.1f}%"

                # è®°å½•ç¼“å­˜ä½¿ç”¨è¯¦æƒ…
                data.update({
                    'used_model': f"{model_data}_{model_arch}",
                    'cache_details': {
                        'shared': {
                            'cached': shared_cached,
                            'path': shared_cache_path if shared_cached else None,
                            'accuracy': self.model_cache.get(shared_cache_path, {}).get('accuracy')
                        },
                        'specific': {
                            'cached': specific_cached,
                            'path': specific_cache_path if specific_cached else None,
                            'accuracy': self.model_cache.get(specific_cache_path, {}).get('accuracy')
                        }
                    }
                })
                output = model(input_tensor)
                return data, output
                
            except Exception as e:
                print(f"âš ï¸ Edge inference failed: {str(e)}")
        else:
            print("æ¨ç†è¾“å…¥é”™è¯¯ï¼Œè¿›è¡Œä¸‹ä¸€æ¬¡å¤„ç†")
            return None

    def _load_full_edge_model(self, shared_path: str, specific_path: str, model_arch, specific_metadata: str) -> nn.Module:
        # åŠ è½½è¾¹ç¼˜èŠ‚ç‚¹ç¼“å­˜çš„å®Œæ•´æ¨¡å‹
        model, config = create_model(model_arch, 2)
        model.shared_layers.load_state_dict(torch.load(shared_path))
        model.specific_layers.load_state_dict(torch.load(specific_path))

        print(f"âœ… Loading full cached model: {model_arch}")

        return model

    def _assemble_hybrid_model(self, cached_components, model_arch, registry_key) -> nn.Module:
        """åŠ¨æ€ç»„è£…æ··åˆæ¨¡å‹ï¼ˆç¼“å­˜å…±äº«å±‚+ä¸‹è½½ç‰¹å®šå±‚ï¼‰"""
        print(f"ğŸ”„ Assembling hybrid model")
        new_model, config = create_model(model_arch, 2)

        #è¾¹ç¼˜ä¾§çš„å…±äº«æƒé‡å­˜åœ¨
        if cached_components['shared']:
            shared_path = cached_components['shared']
        else:
            #TODO:TZY å¢åŠ ä»äº‘ç«¯å¤„ç†è·å–æ–‡ä»¶çš„å»¶è¿Ÿç­‰ä¿¡æ¯
            shared_path = self.cloud_manager.model_db[registry_key]['shared_path']

        if cached_components['specific']:
            specific_path = cached_components['specific']
        else:
            #TODO:TZY å¢åŠ ä»äº‘ç«¯å¤„ç†è·å–æ–‡ä»¶çš„å»¶è¿Ÿç­‰ä¿¡æ¯
            specific_path = self.cloud_manager.model_db[registry_key]['specific_path']

        
        new_model.shared_layers.load_state_dict(torch.load(shared_path))
        new_model.specific_layers.load_state_dict(torch.load(specific_path))
        
        return new_model
    
    def _cache_specific_layer(self, model_key: str, cloud_path: str):
        """ç¼“å­˜ç‰¹å®šå±‚åˆ°æœ¬åœ°ï¼ˆè‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼‰"""
        specific_filename = os.path.basename(cloud_path)
        local_path = os.path.join(self.cache_dir, "specific", specific_filename)
        os.makedirs(os.path.dirname(local_path), exist_ok=True)

        # å¦‚æœæœ¬åœ°æ–‡ä»¶å·²å­˜åœ¨ä¸”å†…å®¹å®Œæ•´ï¼Œç›´æ¥è·³è¿‡
        if os.path.exists(local_path):
            print(f"âœ… Specific layer already cached: {local_path}")
        else:
            # è¿œç¨‹è·¯å¾„ä¸‹è½½
            if any(cloud_path.startswith(prefix) for prefix in ['http://', 'https://', 's3://']):
                self._download_file(cloud_path, local_path)
            # æœ¬åœ°è·¯å¾„å¤åˆ¶ï¼ˆéœ€æ£€æŸ¥æ˜¯å¦ç›¸åŒæ–‡ä»¶ï¼‰
            elif os.path.abspath(cloud_path) != os.path.abspath(local_path):
                shutil.copy(cloud_path, local_path)

        # æ›´æ–°ç¼“å­˜è®°å½•
        self.model_cache.setdefault(model_key, {})['specific_path'] = local_path

    def is_model_cached(self, model_id):
        for registry_key, model_info in self.model_cache.items():
            #æ£€æŸ¥å½“å‰æ¡ç›®çš„'id'æ˜¯å¦åŒ¹é…ç›®æ ‡model_id
            if model_info['id'] == model_id:
                # print("è¾¹ç¼˜èŠ‚ç‚¹ä¿å­˜äº†æ¨¡å‹: ",model_id)
                return True
        return False